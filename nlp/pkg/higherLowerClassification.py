# script for classifying higher and lower order querys

# -*- coding: utf-8 -*-
"""BERT_Questions_higher_order_lower_order.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1JGOJpq6WiBsyLR7qpvc_76xeXwbB20Uw
"""

import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW

# Load data from CSV file
df = pd.read_csv('/content/sample_data/Higher_order_Lower_order.csv')

# Shuffle the rows
df = df.sample(frac=1).reset_index(drop=True)

# Split data into training and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['label'], test_size=0.2)

# Load scientific BERT tokenizer and encode data
tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', do_lower_case=True)

train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)
val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True)

train_labels = torch.tensor(train_labels.tolist())
val_labels = torch.tensor(val_labels.tolist())

# Create scientific BERT model and set to training mode
model = AutoModelForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased', num_labels=2)
model.train()

# Set up optimizer and learning rate scheduler
optimizer = AdamW(model.parameters(), lr=2e-5)

# Train model for 3 epochs
for epoch in range(3):
    print(f"Epoch {epoch + 1}/3")
    for i in range(0, len(train_encodings["input_ids"]), 32):
        batch_encodings = {
            key: torch.tensor(val[i:i+32]) for key, val in train_encodings.items()
        }
        batch_labels = torch.tensor(train_labels[i:i+32])
        outputs = model(**batch_encodings, labels=batch_labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    # Evaluate model after each epoch
    model.eval()
    total_eval_accuracy = 0
    total_eval_loss = 0
    nb_eval_steps = 0
    for i in range(0, len(val_encodings["input_ids"]), 32):
        batch_encodings = {
            key: torch.tensor(val_encodings[key][i:i+32]) for key in val_encodings
        }
        batch_labels = torch.tensor(val_labels[i:i+32])
        with torch.no_grad():
            outputs = model(**batch_encodings, labels=batch_labels)
            loss = outputs.loss
            logits = outputs.logits
        total_eval_loss += loss.item()
        probs = torch.softmax(logits, dim=1)
        preds = torch.argmax(probs, dim=1)
        total_eval_accuracy += torch.sum(preds == batch_labels).item()
        nb_eval_steps += 1
    avg_val_accuracy = total_eval_accuracy / len(val_labels)
    avg_val_loss = total_eval_loss / nb_eval_steps
    print(f"Validation Accuracy: {avg_val_accuracy:.2f}")
    print(f"Validation Loss: {avg_val_loss:.2f}")
    model.train()

# Load BERT tokenizer and encode text
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
text = "What is the difference between velocity and acceleration?"
encodings = tokenizer(text, truncation=True, padding=True, return_tensors='pt')

# Pass the encodings to the model for inference
with torch.no_grad():
    outputs = model(**encodings)
    logits = outputs.logits

# Convert logits to probabilities and get the predicted label
probs = torch.softmax(logits, dim=1)
pred_label = torch.argmax(probs, dim=1)

print(f"Predicted label: {pred_label.item()}")